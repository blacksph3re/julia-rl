{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: \">\" is not a unary operator",
     "output_type": "error",
     "traceback": [
      "syntax: \">\" is not a unary operator",
      ""
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Zygote\n",
    "using PyCall\n",
    "using Pipe\n",
    "using DataStructures\n",
    "using StatsBase\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym = pyimport(\"gym\")\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const STATE_SPACE = length(env.observation_space.low)\n",
    "const ACTION_SPACE = 2\n",
    "const ACTIONS = 0:(ACTION_SPACE-1)\n",
    "const EPSILON_START = 0.10\n",
    "const EPOCHS = 100\n",
    "const STEPS_PER_EPOCH = 5000\n",
    "const TRAIN_STEPS_PER_EPOCH = 2500\n",
    "const BUFFER_SIZE = 10000\n",
    "const BATCH_SIZE = 16\n",
    "const DISCOUNT = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = CircularBuffer{Any}(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Chain(\n",
    "    Dense(STATE_SPACE + ACTION_SPACE, 60, tanh),\n",
    "    LayerNorm(60),\n",
    "    Dense(60, 60, tanh),\n",
    "    LayerNorm(60),\n",
    "    Dense(60, 1),\n",
    ")\n",
    "\n",
    "function forward(model, state, action)\n",
    "    # Onehot has different interfaces for arrays and single values\n",
    "    onehot = length(action) == 1 ? Flux.onehot : Flux.onehotbatch\n",
    "    model(vcat(state, onehot(action, ACTIONS)))\n",
    "end\n",
    "\n",
    "# Act after an epsilon-greedy strategy\n",
    "function act(model, state, epsilon)\n",
    "    if epsilon != 0 && rand() <= epsilon\n",
    "        return rand(ACTIONS)\n",
    "    end\n",
    "    return ACTIONS[argmax(map((a) -> forward(model, state, a), ACTIONS))]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = ADAM(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function grad_descend!(model, parameters, memory)\n",
    "    outer_loss = 0\n",
    "    # Sample BATCH_SIZE elements from the replay buffer\n",
    "    batch = memory[1:BATCH_SIZE]\n",
    "    sample!(memory, batch, replace=false)\n",
    "    # Destructure the batch\n",
    "    state, action, reward, next_state, death = [getindex.(batch, i) for i in 1:5]\n",
    "    state = hcat(state...)\n",
    "    next_state = hcat(next_state...)\n",
    "\n",
    "    # Perform one step of grad descend\n",
    "    Q_next = (.!death) .* [maximum(map((a) -> forward(model, s, a), ACTIONS))[1] for s in eachcol(next_state)]\n",
    "    target = reward .+ DISCOUNT*Q_next\n",
    "\n",
    "    grads = gradient(parameters) do\n",
    "        loss = Flux.mse(forward(model, state, action), target)\n",
    "        outer_loss += loss\n",
    "        return loss\n",
    "    end\n",
    "\n",
    "    Flux.update!(optim, parameters, grads)\n",
    "    return outer_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train_epoch!(memory, model, env, epoch)\n",
    "    state = env.reset()\n",
    "    traj_rewards = []\n",
    "    total_reward = 0\n",
    "    epsilon = EPSILON_START * (1 - epoch/EPOCHS)\n",
    "    \n",
    "    parameters = Flux.params(model)\n",
    "    total_loss = 0.0\n",
    "    \n",
    "   \n",
    "    for step in 1:STEPS_PER_EPOCH\n",
    "        action = act(model, state, epsilon) # Act\n",
    "        next_state, reward, death, _ = env.step(action) # Advance the env\n",
    "        push!(memory, (state, action, reward, next_state, death)) # Save that memory\n",
    "        total_reward += reward\n",
    "        if death\n",
    "            state = env.reset()\n",
    "            push!(traj_rewards, total_reward)\n",
    "            total_reward = 0\n",
    "        else\n",
    "            state = next_state\n",
    "        end\n",
    "        \n",
    "        if length(memory) > BATCH_SIZE\n",
    "            loss = grad_descend!(model, parameters, memory)\n",
    "            total_loss += loss\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return mean(traj_rewards), total_loss / STEPS_PER_EPOCH\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in 1:EPOCHS\n",
    "    reward, loss = train_epoch!(memory, model, env, e)\n",
    "    @printf(\"Epoch %d, Loss %f, Reward %f\\n\", e, reward, loss)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element BitArray{1}:\n",
       " 0\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test(model, env)\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for i in 1:STEPS_PER_EPOCH\n",
    "        action = act(model, state, 0)\n",
    "        state, reward, death, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        env.render()\n",
    "        death && return total_reward, i\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "s = 0\n",
    "r = 0\n",
    "d = 0\n",
    "for _ in 1:5\n",
    "    s,r,d = env.step(0)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Float32,1}:\n",
       " 9.298442"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward(model, s, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
