{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Zygote\n",
    "using PyCall\n",
    "using DataStructures\n",
    "using StatsBase\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using DistributionsAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 18 entries:\n",
       "  \"value_layer2\"         => 30\n",
       "  \"lr\"                   => 0.0001\n",
       "  \"policy_layer2\"        => 30\n",
       "  \"discount_factor\"      => 0.99\n",
       "  \"train_steps_per_iter\" => 2\n",
       "  \"batch_size\"           => 64\n",
       "  \"steps_per_epoch\"      => 2000\n",
       "  \"env\"                  => \"Pendulum-v0\"\n",
       "  \"buffer_size\"          => 100000\n",
       "  \"l2_reg\"               => 0.0001\n",
       "  \"q_layer1\"             => 30\n",
       "  \"target_update\"        => 0.0001\n",
       "  \"policy_layer1\"        => 30\n",
       "  \"value_layer1\"         => 30\n",
       "  \"activation\"           => tanh\n",
       "  \"epochs\"               => 100\n",
       "  \"entropy_incentive\"    => 0.01\n",
       "  \"q_layer2\"             => 30"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams = Dict([\n",
    "    (\"lr\", 1e-4),\n",
    "    (\"env\", \"Pendulum-v0\"),\n",
    "    (\"policy_layer1\", 30),\n",
    "    (\"policy_layer2\", 30),\n",
    "    (\"value_layer1\", 30),\n",
    "    (\"value_layer2\", 30),\n",
    "    (\"q_layer1\", 30),\n",
    "    (\"q_layer2\", 30),\n",
    "    (\"activation\", tanh),\n",
    "    (\"target_update\", 1e-4),\n",
    "    (\"entropy_incentive\", 1e-2),\n",
    "    (\"l2_reg\", 1e-4),\n",
    "    (\"batch_size\", 64),\n",
    "    (\"discount_factor\", 0.99),\n",
    "    (\"buffer_size\", 100000),\n",
    "    (\"epochs\", 100),\n",
    "    (\"steps_per_epoch\", 2000),\n",
    "    (\"train_steps_per_iter\", 2)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym = pyimport(\"gym\")\n",
    "env = gym.make(hparams[\"env\"])\n",
    "\n",
    "\n",
    "STATE_SPACE = length(env.observation_space.low)\n",
    "ACTION_SPACE = length(env.action_space.low)\n",
    "TARGET_UPDATE = hparams[\"target_update\"]\n",
    "ENTROPY_INCENTIVE = hparams[\"entropy_incentive\"]\n",
    "L2_REG = hparams[\"l2_reg\"]\n",
    "BATCH_SIZE = hparams[\"batch_size\"]\n",
    "GAMMA = hparams[\"discount_factor\"]\n",
    "EPOCHS = hparams[\"epochs\"]\n",
    "STEPS_PER_EPOCH = hparams[\"steps_per_epoch\"]\n",
    "TRAIN_STEPS_PER_ITER = hparams[\"train_steps_per_iter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.0001, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim = Flux.ADAM(hparams[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-element CircularBuffer{Tuple{Array{Float32,1},Array{Float32,1},Float32,Array{Float32,1},Bool}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = CircularBuffer{Tuple{Array{Float32,1}, Array{Float32, 1}, Float32, Array{Float32,1}, Bool}}(hparams[\"buffer_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(3, 30, tanh), LayerNorm(30), Dense(30, 30, tanh), LayerNorm(30), Dense(30, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = Chain(\n",
    "    Dense(STATE_SPACE, hparams[\"value_layer1\"], hparams[\"activation\"]),\n",
    "    LayerNorm(hparams[\"value_layer1\"]),\n",
    "    Dense(hparams[\"value_layer1\"], hparams[\"value_layer2\"], hparams[\"activation\"]),\n",
    "    LayerNorm(hparams[\"value_layer2\"]),\n",
    "    Dense(hparams[\"value_layer2\"], 1),\n",
    ")\n",
    "\n",
    "value_target = deepcopy(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward_critic (generic function with 2 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critics = map((_) -> Chain(\n",
    "    Dense(STATE_SPACE+ACTION_SPACE, hparams[\"q_layer1\"], hparams[\"activation\"]),\n",
    "    LayerNorm(hparams[\"q_layer1\"]),\n",
    "    Dense(hparams[\"q_layer1\"], hparams[\"q_layer2\"], hparams[\"activation\"]),\n",
    "    LayerNorm(hparams[\"q_layer2\"]),\n",
    "    Dense(hparams[\"q_layer2\"], 1),\n",
    "), 1:2)\n",
    "\n",
    "\n",
    "# Single value mode\n",
    "function forward_critic(models, state::Array{T,1}, action::Array{T,1}) where {T}\n",
    "    pred = map((model) -> model(vcat(state, action)), models)\n",
    "    minimum(Flux.stack(pred, 1))\n",
    "end\n",
    "# Batch mode\n",
    "function forward_critic(models, state::Array{T,2}, action::Array{T,2}) where {T}\n",
    "    pred = map((model) -> dropdims(model(vcat(state, action)), dims=1), models)\n",
    "    minimum(Flux.stack(pred, 1), dims=1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logprobs (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = Chain(\n",
    "    Dense(STATE_SPACE, hparams[\"policy_layer1\"], hparams[\"activation\"]),\n",
    "    LayerNorm(hparams[\"policy_layer1\"]),\n",
    "    Dense(hparams[\"policy_layer1\"], hparams[\"policy_layer2\"], hparams[\"activation\"]),\n",
    "    LayerNorm(hparams[\"policy_layer2\"]),\n",
    "    Dense(hparams[\"policy_layer2\"], ACTION_SPACE*2),\n",
    ")\n",
    "\n",
    "function forward_policy(model, state::Array{T, 2}) where {T}\n",
    "    x = policy(state)\n",
    "    mean = x[1:ACTION_SPACE,:]\n",
    "    std = exp.(x[ACTION_SPACE+1:end,:])\n",
    "    return mean, std\n",
    "end\n",
    "function forward_policy(model, state::Array{T, 1}) where {T}\n",
    "    x = policy(state)\n",
    "    mean = x[1:ACTION_SPACE]\n",
    "    std = exp.(x[ACTION_SPACE+1:end])\n",
    "    return mean, std\n",
    "end\n",
    "\n",
    "function forward_policy_dist(model, state::Array{T, 1}) where {T}\n",
    "    MvNormal(forward_policy(model, state)...)\n",
    "end\n",
    "function forward_policy_dist(model, state::Array{T, 2}) where {T}\n",
    "    mean, std = forward_policy(model, state)\n",
    "    map((x) -> MvNormal(x[1], x[2]), zip(eachcol(mean), eachcol(std)))\n",
    "end\n",
    "function forward_policy_logprob(model, state, action)\n",
    "    dists = forward_policy_dist(policy, state)\n",
    "    logprob = Flux.batch(map((x) -> logpdf(x[1], x[2]), zip(dists, eachcol(action))))\n",
    "end\n",
    "function logprob(mean, std, action)\n",
    "    logpdf(TuringDiagMvNormal(mean, std), action)\n",
    "end\n",
    "function logprobs(mean, std, actions)\n",
    "    Flux.unsqueeze(map((i) -> logprob(mean[:,i], std[:,i], actions[:,i]), 1:size(actions)[2]), 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_value! (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_value_target!(value_target, value)\n",
    "    for (p, p_target) in zip(Flux.params(value), Flux.params(value_target))\n",
    "       p_target .= (1-TARGET_UPDATE) .* p_target .+ TARGET_UPDATE .* p\n",
    "    end\n",
    "end\n",
    "\n",
    "function update_value!(value, critics, policy, optim, states::Array{T}) where {T}\n",
    "    parameters = Flux.params(value)\n",
    "    outer_loss = 0\n",
    "    \n",
    "    dist = forward_policy_dist(policy, states)\n",
    "    actions = T.(Flux.batch([rand(d) for d in dist]))\n",
    "    p = Flux.batch([logpdf(d, a) for (d,a) in zip(dist, eachcol(actions))])\n",
    "    q = forward_critic(critics, states, actions)\n",
    "    target = q .- ENTROPY_INCENTIVE * p\n",
    "    \n",
    "    #println(mean(target), mean(q), mean(p))\n",
    "    \n",
    "    grads = gradient(parameters) do\n",
    "        loss = Flux.huber_loss(value(states), target) + L2_REG * sum(norm, parameters)\n",
    "        outer_loss += loss\n",
    "        return loss\n",
    "    end\n",
    "\n",
    "    Flux.update!(optim, parameters, grads)\n",
    "    outer_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_critic! (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_critic!(critic, value_target, optim, states, actions, rewards, next_states, deaths)\n",
    "    target = rewards .+ GAMMA .* (.!deaths) .* value_target(next_states)\n",
    "    parameters = Flux.params(critic)\n",
    "    outer_loss = 0\n",
    "    \n",
    "    grads = gradient(parameters) do\n",
    "        loss = Flux.huber_loss(critic(vcat(states, actions)), target)\n",
    "        outer_loss += loss\n",
    "        return loss\n",
    "    end\n",
    "    \n",
    "    Flux.update!(optim, parameters, grads)\n",
    "    outer_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_policy! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_policy!(policy, critics, optim, states::Array{T}) where {T}\n",
    "    parameters = Flux.params(policy)\n",
    "    outer_loss = 0\n",
    "    batch_size = size(states)[2]\n",
    "    unitnoise = T.(rand(Normal(), (ACTION_SPACE, batch_size)))\n",
    "    \n",
    "    grads = gradient(parameters) do\n",
    "        mean, std = forward_policy(policy, states)\n",
    "        actions = unitnoise .* std .+ mean\n",
    "        q = forward_critic(critics, states, actions)\n",
    "        is = ENTROPY_INCENTIVE * logprobs(mean, std, actions)\n",
    "        loss = sum(is .- q) / batch_size\n",
    "        outer_loss += loss\n",
    "        return loss\n",
    "    end\n",
    "    \n",
    "    Flux.update!(optim, parameters, grads)\n",
    "    outer_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch(memory)\n",
    "    batch = vcat([sample(memory) for _ in 1:BATCH_SIZE])\n",
    "    # Destructure the batch\n",
    "    state, action, reward, next_state, death = [getindex.(batch, i) for i in 1:5]\n",
    "    state = Flux.batch(state)\n",
    "    next_state = Flux.batch(next_state)\n",
    "    action = Flux.batch(action)\n",
    "    return state, action, reward, next_state, death\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I: 1, r: -601.944580, v: 0.108978, q1: 2.885548, q2: 2.887355, p: 4.719818\n",
      "I: 2, r: -901.188904, v: 0.432230, q1: 2.852739, q2: 2.855324, p: 3.707046\n",
      "I: 3, r: -684.379639, v: 0.409662, q1: 2.711770, q2: 2.716033, p: 3.869687\n",
      "I: 4, r: -505.849304, v: 0.465882, q1: 2.774301, q2: 2.777215, p: 3.967479\n",
      "I: 5, r: -833.089539, v: 0.457431, q1: 2.757236, q2: 2.759940, p: 4.043257\n",
      "I: 6, r: -761.569580, v: 0.450687, q1: 2.659956, q2: 2.662855, p: 4.220744\n",
      "I: 7, r: -645.099609, v: 0.448799, q1: 2.594251, q2: 2.595930, p: 4.325452\n",
      "I: 8, r: -571.018677, v: 0.455499, q1: 2.671499, q2: 2.672621, p: 4.318037\n",
      "I: 9, r: -755.985779, v: 0.413188, q1: 2.635601, q2: 2.636891, p: 4.340734\n",
      "I: 10, r: -582.337952, v: 0.340242, q1: 2.580169, q2: 2.580998, p: 4.404067\n"
     ]
    }
   ],
   "source": [
    "for epoch in 1:EPOCHS\n",
    "    total_reward = 0\n",
    "    total_deaths = 0\n",
    "    total_v_loss = 0\n",
    "    total_q1_loss = 0\n",
    "    total_q2_loss = 0\n",
    "    total_policy_loss = 0\n",
    "    total_iterations = 0\n",
    "    state = Float32.(env.reset())\n",
    "    for i in 1:STEPS_PER_EPOCH\n",
    "        action = rand(forward_policy_dist(policy, state)) # Act\n",
    "        action = clamp(action, env.action_space.low, env.action_space.high)\n",
    "        next_state, reward, death, _ = env.step(action) # Advance the env\n",
    "\n",
    "        # Convert to Float32\n",
    "        next_state = Float32.(next_state)\n",
    "        reward = Float32(reward)\n",
    "\n",
    "        push!(memory, (state, action, reward, next_state, death))\n",
    "        total_reward += reward\n",
    "        if death\n",
    "            state = env.reset()\n",
    "            total_deaths += 1\n",
    "        else\n",
    "            state = next_state\n",
    "        end\n",
    "        \n",
    "        if length(memory) > BATCH_SIZE\n",
    "            for i in 1:TRAIN_STEPS_PER_ITER\n",
    "                states, actions, rewards, next_states, deaths = batch(memory)\n",
    "                total_v_loss += update_value!(value, critics, policy, optim, states)\n",
    "                total_q1_loss += update_critic!(critics[1], value_target, optim, states, actions, rewards, next_states, deaths)\n",
    "                total_q2_loss += update_critic!(critics[2], value_target, optim, states, actions, rewards, next_states, deaths)\n",
    "                total_policy_loss += update_policy!(policy, critics, optim, states)\n",
    "                update_value_target!(value_target, value)\n",
    "                total_iterations += 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    println(@sprintf(\"I: %d, r: %f, v: %f, q1: %f, q2: %f, p: %f\", \n",
    "            epoch,\n",
    "            total_reward/(total_deaths+1),\n",
    "            total_v_loss/total_iterations, \n",
    "            total_q1_loss/total_iterations, \n",
    "            total_q2_loss/total_iterations, \n",
    "            total_policy_loss/total_iterations))\n",
    "    flush(stdout)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
