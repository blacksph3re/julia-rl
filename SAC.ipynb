{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Zygote\n",
    "using PyCall\n",
    "using DataStructures\n",
    "using StatsBase\n",
    "using Printf\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using DistributionsAD\n",
    "using Test\n",
    "using BSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 18 entries:\n",
       "  \"value_layer2\"         => 32\n",
       "  \"lr\"                   => 0.001\n",
       "  \"policy_layer2\"        => 32\n",
       "  \"discount_factor\"      => 0.99\n",
       "  \"train_steps_per_iter\" => 2\n",
       "  \"batch_size\"           => 100\n",
       "  \"steps_per_epoch\"      => 2000\n",
       "  \"env\"                  => \"CartPoleContinuousBulletEnv-v0\"\n",
       "  \"buffer_size\"          => 1000000\n",
       "  \"l2_reg\"               => 0.0001\n",
       "  \"q_layer1\"             => 32\n",
       "  \"target_update\"        => 0.001\n",
       "  \"policy_layer1\"        => 32\n",
       "  \"value_layer1\"         => 32\n",
       "  \"activation\"           => swish\n",
       "  \"epochs\"               => 100\n",
       "  \"entropy_incentive\"    => 0.2\n",
       "  \"q_layer2\"             => 32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_size = 32\n",
    "\n",
    "hparams = Dict([\n",
    "    (\"lr\", 1e-3),\n",
    "    (\"env\", \"CartPoleContinuousBulletEnv-v0\"),\n",
    "    (\"policy_layer1\", layer_size),\n",
    "    (\"policy_layer2\", layer_size),\n",
    "    (\"value_layer1\", layer_size),\n",
    "    (\"value_layer2\", layer_size),\n",
    "    (\"q_layer1\", layer_size),\n",
    "    (\"q_layer2\", layer_size),\n",
    "    (\"activation\", swish),\n",
    "    (\"target_update\", 1e-3),\n",
    "    (\"entropy_incentive\", 0.2),\n",
    "    (\"l2_reg\", 1e-4),\n",
    "    (\"batch_size\", 100),\n",
    "    (\"discount_factor\", 0.99),\n",
    "    (\"buffer_size\", 1000000),\n",
    "    (\"epochs\", 100),\n",
    "    (\"steps_per_epoch\", 2000),\n",
    "    (\"train_steps_per_iter\", 2)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jul  8 2020 18:24:12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyimport(\"pybullet_envs\")\n",
    "gym = pyimport(\"gym\")\n",
    "env = gym.make(hparams[\"env\"])\n",
    "\n",
    "\n",
    "STATE_SPACE = length(env.observation_space.low)\n",
    "ACTION_SPACE = length(env.action_space.low)\n",
    "ACTION_HIGH = env.action_space.high\n",
    "ACTION_LOW = env.action_space.low\n",
    "TARGET_UPDATE = hparams[\"target_update\"]\n",
    "ENTROPY_INCENTIVE = hparams[\"entropy_incentive\"]\n",
    "L2_REG = hparams[\"l2_reg\"]\n",
    "BATCH_SIZE = hparams[\"batch_size\"]\n",
    "GAMMA = hparams[\"discount_factor\"]\n",
    "EPOCHS = hparams[\"epochs\"]\n",
    "STEPS_PER_EPOCH = hparams[\"steps_per_epoch\"]\n",
    "TRAIN_STEPS_PER_ITER = hparams[\"train_steps_per_iter\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim = Flux.ADAM(hparams[\"lr\"])\n",
    "dtype = Float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "support_to_action (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our squashed policy has a support of -1,1\n",
    "# Project actions to/from that support\n",
    "function action_to_support(action)\n",
    "    halfspan = (ACTION_HIGH .- ACTION_LOW) ./ 2\n",
    "    low_end = ACTION_LOW ./ halfspan\n",
    "    action ./ halfspan .- low_end .- 1\n",
    "end\n",
    "\n",
    "function support_to_action(action)\n",
    "    halfspan = (ACTION_HIGH .- ACTION_LOW) ./ 2\n",
    "    low_end = ACTION_LOW ./ halfspan\n",
    "    (action .+ 1 .+ low_end) .* halfspan\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin\n",
    "    local lol1 = Flux.batch([rand(ACTION_SPACE) for _ in 1:100])\n",
    "    @test isapprox(lol1, support_to_action(action_to_support(lol1)))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialize (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initialize()\n",
    "    \n",
    "    typeswitch(T, x) = x\n",
    "    typeswitch(T, x::Number) = T(x)\n",
    "    typeswitch(T, x::AbstractArray) = T.(x)\n",
    "    \n",
    "    value = Chain(\n",
    "        Dense(STATE_SPACE, hparams[\"value_layer1\"], hparams[\"activation\"]),\n",
    "        #LayerNorm(hparams[\"value_layer1\"]),\n",
    "        Dense(hparams[\"value_layer1\"], hparams[\"value_layer2\"], hparams[\"activation\"]),\n",
    "        #LayerNorm(hparams[\"value_layer2\"]),\n",
    "        Dense(hparams[\"value_layer2\"], 1),\n",
    "    )\n",
    "    value = Flux.fmap(x -> typeswitch(dtype, x), value)\n",
    "\n",
    "    value_target = deepcopy(value)\n",
    "\n",
    "    critics = map((_) -> Flux.fmap(x -> typeswitch(dtype, x), Chain(\n",
    "        Dense(STATE_SPACE+ACTION_SPACE, hparams[\"q_layer1\"], hparams[\"activation\"]),\n",
    "        #LayerNorm(hparams[\"q_layer1\"]),\n",
    "        Dense(hparams[\"q_layer1\"], hparams[\"q_layer2\"], hparams[\"activation\"]),\n",
    "        #LayerNorm(hparams[\"q_layer2\"]),\n",
    "        Dense(hparams[\"q_layer2\"], 1),\n",
    "    )), 1:2)\n",
    "\n",
    "    policy = Chain(\n",
    "        Dense(STATE_SPACE, hparams[\"policy_layer1\"], hparams[\"activation\"]),\n",
    "        #LayerNorm(hparams[\"policy_layer1\"]),\n",
    "        Dense(hparams[\"policy_layer1\"], hparams[\"policy_layer2\"], hparams[\"activation\"]),\n",
    "        #LayerNorm(hparams[\"policy_layer2\"]),\n",
    "        Dense(hparams[\"policy_layer2\"], ACTION_SPACE*2),\n",
    "    )\n",
    "    policy = Flux.fmap(x -> typeswitch(dtype, x), policy)\n",
    "\n",
    "    \n",
    "    memory = memory = CircularBuffer{Tuple{Array{dtype,1}, Array{dtype, 1}, dtype, Array{dtype,1}, Bool}}(hparams[\"buffer_size\"])\n",
    "    \n",
    "    value, value_target, critics, policy, memory\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward_critic (generic function with 2 methods)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Single value mode\n",
    "function forward_critic(models, state::Array{T,1}, action::Array{T,1}) where {T}\n",
    "    pred = map((model) -> model(vcat(state, action)), models)\n",
    "    minimum(Flux.stack(pred, 1))\n",
    "end\n",
    "# Batch mode\n",
    "function forward_critic(models, state::Array{T,2}, action::Array{T,2}) where {T}\n",
    "    pred = map((model) -> dropdims(model(vcat(state, action)), dims=1), models)\n",
    "    minimum(Flux.stack(pred, 1), dims=1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logprobs (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "function forward_policy(model, state::Array{T, 2}) where {T}\n",
    "    x = policy(state)\n",
    "    mu = x[1:ACTION_SPACE,:]\n",
    "    sigma = exp.(x[ACTION_SPACE+1:end,:])\n",
    "    return mu, sigma\n",
    "end\n",
    "function forward_policy(model, state::Array{T, 1}) where {T}\n",
    "    x = policy(state)\n",
    "    mu = x[1:ACTION_SPACE]\n",
    "    sigma = exp.(x[ACTION_SPACE+1:end])\n",
    "    return mu, sigma\n",
    "end\n",
    "\n",
    "# The SAC authors squash their action space\n",
    "# See Appendix C in their paper\n",
    "function squash(actions)\n",
    "    tanh.(actions)\n",
    "end\n",
    "function policy_sample(mu::Array{T}, sigma::Array{T}) where {T}\n",
    "    mu .+ sigma .* clamp.(T.(rand(Normal(), size(sigma))), -10, 10)\n",
    "end\n",
    "function policy_sample(model, state::Array)\n",
    "    policy_sample(forward_policy(model, state)...)\n",
    "end\n",
    "function act(model, state)\n",
    "    support_to_action(squash(policy_sample(model, state)))\n",
    "end\n",
    "function logprob(mu, sigma, action_unsquashed::Array{T}) where {T}\n",
    "    # Add a small epsilon so we don't explode the loss\n",
    "    logpdf(TuringDiagMvNormal(mu, sigma), action_unsquashed) - sum(log.(1 .- tanh.(action_unsquashed) .^ 2 .+ eps(T) ))\n",
    "end\n",
    "function logprobs(mu, sigma, actions_unsquashed)\n",
    "    Flux.unsqueeze(map((i) -> logprob(mu[:,i], sigma[:,i], actions_unsquashed[:,i]), 1:size(actions_unsquashed)[2]), 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_value! (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_value_target!(value_target, value)\n",
    "    for (p, p_target) in zip(Flux.params(value), Flux.params(value_target))\n",
    "        p_target .= (1-TARGET_UPDATE) .* p_target .+ TARGET_UPDATE .* p\n",
    "        @assert !any(isnan.(p_target))\n",
    "    end\n",
    "end\n",
    "\n",
    "function update_value!(value, critics, policy, optim, states::Array{T}) where {T}\n",
    "    parameters = Flux.params(value)\n",
    "    outer_loss = 0\n",
    "    \n",
    "    mu, sigma = forward_policy(policy, states)\n",
    "    actions = policy_sample(mu, sigma)\n",
    "    p = logprobs(mu, sigma, actions)\n",
    "    q = forward_critic(critics, states, squash(actions))\n",
    "    target = q .- ENTROPY_INCENTIVE .* p\n",
    "    \n",
    "    \n",
    "    grads = gradient(parameters) do\n",
    "        loss = Flux.mse(value(states), target)# + L2_REG * sum(norm, parameters)\n",
    "        outer_loss += loss\n",
    "        return loss\n",
    "    end\n",
    "    \n",
    "    Flux.update!(optim, parameters, grads)\n",
    "    @assert !any([any(isnan.(layer.W)) for layer in value])\n",
    "    outer_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_critic! (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_critic!(critic, value_target, optim, states, actions, rewards, next_states, deaths)\n",
    "    parameters = Flux.params(critic)\n",
    "    outer_loss = 0\n",
    "    target = rewards .+ GAMMA .* (.!deaths) .* dropdims(value_target(next_states), dims=1)\n",
    "    @assert !any(isnan.(target))\n",
    "        \n",
    "    grads = gradient(parameters) do\n",
    "        loss = Flux.mse(critic(vcat(states, actions)), target)# + L2_REG * sum(norm, parameters)\n",
    "        outer_loss += loss\n",
    "        return loss\n",
    "    end\n",
    "    \n",
    "    Flux.update!(optim, parameters, grads)\n",
    "    \n",
    "    @assert !any([any(isnan.(layer.W)) for layer in critic])\n",
    "    outer_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_policy! (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_policy!(policy, critics, optim, states::Array{T}) where {T}\n",
    "    parameters = Flux.params(policy)\n",
    "    outer_loss = 0\n",
    "    batch_size = size(states)[2]\n",
    "    unitnoise = T.(rand(Normal(), (ACTION_SPACE, batch_size)))\n",
    "    unitnoise = clamp.(unitnoise, -10, 10) # Don't allow crazy outliers\n",
    "    \n",
    "    grads = gradient(parameters) do\n",
    "        mu, sigma = forward_policy(policy, states)\n",
    "        actions = mu .+ sigma .* unitnoise\n",
    "        q = forward_critic(critics, states, squash(actions))\n",
    "        is = ENTROPY_INCENTIVE * logprobs(mu, sigma, actions)\n",
    "        loss = sum(is .- q) / batch_size #+ L2_REG * sum(norm, parameters)\n",
    "        outer_loss += loss\n",
    "        return loss\n",
    "    end\n",
    "        \n",
    "    Flux.update!(optim, parameters, grads)\n",
    "    @assert !any([any(isnan.(layer.W)) for layer in policy])\n",
    "    outer_loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch(memory)\n",
    "    batch = vcat([sample(memory) for _ in 1:BATCH_SIZE])\n",
    "    \n",
    "    # Destructure the batch\n",
    "    state, action, reward, next_state, death = [getindex.(batch, i) for i in 1:5]\n",
    "    state = Flux.batch(state)\n",
    "    next_state = Flux.batch(next_state)\n",
    "    action = Flux.batch(action)\n",
    "    \n",
    "    @assert !any(isnan.(state))\n",
    "    @assert !any(isnan.(action))\n",
    "    \n",
    "    return state, action, reward, next_state, death\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collect_experience! (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function collect_experience!(env, memory, steps)\n",
    "  state = dtype.(env.reset())\n",
    "  total_reward = 0\n",
    "  total_deaths = 0\n",
    "  for _ in 1:steps\n",
    "    action = dtype.(clamp(rand(MvNormal(ACTION_LOW, ACTION_HIGH)), ACTION_LOW, ACTION_HIGH))\n",
    "    next_state, reward, death, _ = env.step(action) # Advance the env\n",
    "\n",
    "    # Convert to dtype\n",
    "    next_state = dtype.(next_state)\n",
    "    reward = dtype(reward)\n",
    "    total_reward += reward\n",
    "\n",
    "    push!(memory, (state, action, reward, next_state, death))\n",
    "    \n",
    "    if death\n",
    "        state = env.reset()\n",
    "        total_deaths += 1\n",
    "    end\n",
    "  end\n",
    "  total_reward / (total_deaths + 1)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "function train!(env, memory, optim, policy, critics, value, value_target)\n",
    "  for epoch in 1:EPOCHS\n",
    "    total_reward = 0\n",
    "    total_deaths = 0\n",
    "    total_v_loss = 0\n",
    "    total_q1_loss = 0\n",
    "    total_q2_loss = 0\n",
    "    total_policy_loss = 0\n",
    "    total_entropy = 0\n",
    "    total_iterations = 0\n",
    "    state = dtype.(env.reset())\n",
    "    for i in 1:STEPS_PER_EPOCH\n",
    "        action = act(policy, state) # Act\n",
    "        next_state, reward, death, _ = env.step(action) # Advance the env\n",
    "\n",
    "        # Convert to Float32\n",
    "        next_state = dtype.(next_state)\n",
    "        reward = dtype(reward)\n",
    "\n",
    "        push!(memory, (state, action, reward, next_state, death))\n",
    "        total_reward += reward\n",
    "        if death\n",
    "            state = env.reset()\n",
    "            total_deaths += 1\n",
    "        else\n",
    "            state = next_state\n",
    "        end\n",
    "\n",
    "        if length(memory) > BATCH_SIZE\n",
    "            for i in 1:1\n",
    "                states, actions, rewards, next_states, deaths = batch(memory)\n",
    "                actions = action_to_support(actions)\n",
    "                total_v_loss += update_value!(value, critics, policy, optim, states)\n",
    "                total_q1_loss += update_critic!(critics[1], value_target, optim, states, actions, rewards, next_states, deaths)\n",
    "                total_q2_loss += update_critic!(critics[2], value_target, optim, states, actions, rewards, next_states, deaths)\n",
    "                total_policy_loss += update_policy!(policy, critics, optim, states)\n",
    "                update_value_target!(value_target, value)\n",
    "                @assert(!isnan(total_v_loss))\n",
    "                @assert(!isnan(total_q1_loss))\n",
    "                @assert(!isnan(total_q2_loss))\n",
    "                @assert(!isnan(total_policy_loss))\n",
    "                total_iterations += 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    s,a,r,sn,d = batch(memory)\n",
    "    v = mean(value(s))\n",
    "    q1 = mean(critics[1](vcat(s, a)))\n",
    "    q2 = mean(critics[2](vcat(s, a)))\n",
    "    mu, sigma = forward_policy(policy, s)\n",
    "    println(@sprintf(\"v: %f, q1: %f, q2: %f, e: %f\", \n",
    "            v, \n",
    "            q1,\n",
    "            q2,\n",
    "            mean(sigma)))\n",
    "\n",
    "    println(@sprintf(\"I: %d, r: %f, v: %f, q1: %f, q2: %f, p: %f\",\n",
    "            epoch,\n",
    "            total_reward/(total_deaths+1),\n",
    "            total_v_loss/total_iterations,\n",
    "            total_q1_loss/total_iterations,\n",
    "            total_q2_loss/total_iterations,\n",
    "            total_policy_loss/total_iterations))\n",
    "    flush(stdout)\n",
    "  end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.526315789473685"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value, value_target, critics, policy, memory = initialize()\n",
    "collect_experience!(env, memory, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v: 52.257338, q1: 51.800230, q2: 52.701609, e: 0.868286\n",
      "I: 1, r: 27.027027, v: 0.148652, q1: 11168.949959, q2: 11172.054303, p: -53.700926\n",
      "v: 49.196464, q1: 48.331616, q2: 49.122526, e: 0.854429\n",
      "I: 2, r: 24.096386, v: 0.045238, q1: 10289.173995, q2: 10291.071166, p: -50.713589\n",
      "v: 46.969230, q1: 46.194891, q2: 47.114251, e: 0.868411\n",
      "I: 3, r: 22.988506, v: 0.027596, q1: 8991.419835, q2: 8992.918227, p: -48.119232\n",
      "v: 45.065706, q1: 44.254430, q2: 45.050210, e: 0.846939\n",
      "I: 4, r: 26.315789, v: 0.021713, q1: 8186.006575, q2: 8187.571418, p: -45.722570\n",
      "v: 42.767125, q1: 41.732006, q2: 42.320523, e: 0.855683\n",
      "I: 5, r: 26.315789, v: 0.021374, q1: 7439.763417, q2: 7440.917655, p: -43.561024\n",
      "v: 40.551669, q1: 39.870996, q2: 40.733340, e: 0.867525\n",
      "I: 6, r: 25.641026, v: 0.024377, q1: 6716.913646, q2: 6718.077717, p: -41.604275\n",
      "v: 38.531498, q1: 37.937041, q2: 38.589101, e: 0.858684\n",
      "I: 7, r: 25.316456, v: 0.023610, q1: 6004.096084, q2: 6005.311593, p: -39.920625\n",
      "v: 37.512623, q1: 37.555227, q2: 38.153246, e: 0.889469\n",
      "I: 8, r: 27.027027, v: 0.017995, q1: 5640.833060, q2: 5641.841951, p: -38.329842\n",
      "v: 35.929423, q1: 35.229752, q2: 35.911493, e: 0.872302\n",
      "I: 9, r: 25.000000, v: 0.018774, q1: 5333.835787, q2: 5334.878785, p: -36.814554\n",
      "v: 35.292286, q1: 35.030767, q2: 35.669802, e: 0.878215\n",
      "I: 10, r: 25.000000, v: 0.019673, q1: 4722.782280, q2: 4723.920391, p: -35.593002\n",
      "v: 34.122999, q1: 33.360716, q2: 34.002688, e: 0.857764\n",
      "I: 11, r: 26.666667, v: 0.020563, q1: 4439.682720, q2: 4440.595328, p: -34.442312\n",
      "v: 33.117577, q1: 32.617869, q2: 33.287671, e: 0.870812\n",
      "I: 12, r: 27.777778, v: 0.015230, q1: 4226.190903, q2: 4226.814712, p: -33.399757\n",
      "v: 31.684368, q1: 31.075694, q2: 31.598895, e: 0.862491\n",
      "I: 13, r: 27.397260, v: 0.018488, q1: 4024.697461, q2: 4025.724143, p: -32.425079\n",
      "v: 31.216292, q1: 31.219742, q2: 31.975451, e: 0.877915\n",
      "I: 14, r: 25.974026, v: 0.017282, q1: 3798.447293, q2: 3799.322099, p: -31.529459\n",
      "v: 30.696355, q1: 30.057346, q2: 30.515462, e: 0.870563\n",
      "I: 15, r: 25.974026, v: 0.018812, q1: 3499.400885, q2: 3500.354126, p: -30.777988\n",
      "v: 30.074925, q1: 29.561390, q2: 30.190206, e: 0.866988\n",
      "I: 16, r: 25.316456, v: 0.016766, q1: 3363.631552, q2: 3364.216226, p: -30.061880\n",
      "v: 29.076217, q1: 27.838810, q2: 28.223135, e: 0.864084\n",
      "I: 17, r: 28.169014, v: 0.016597, q1: 3218.827740, q2: 3219.549581, p: -29.447414\n",
      "v: 29.023322, q1: 28.375234, q2: 28.856454, e: 0.868254\n",
      "I: 18, r: 28.985507, v: 0.027996, q1: 3136.457483, q2: 3137.290024, p: -28.837579\n",
      "v: 27.981621, q1: 27.528525, q2: 27.993922, e: 0.871248\n",
      "I: 19, r: 25.641026, v: 0.020830, q1: 2957.834000, q2: 2958.529461, p: -28.331983\n",
      "v: 27.632403, q1: 27.264550, q2: 27.823076, e: 0.856074\n",
      "I: 20, r: 25.316456, v: 0.020851, q1: 2871.310189, q2: 2872.028817, p: -27.862952\n",
      "v: 27.154711, q1: 26.526512, q2: 27.024235, e: 0.865253\n",
      "I: 21, r: 27.777778, v: 0.019584, q1: 2789.963661, q2: 2790.539685, p: -27.426076\n",
      "v: 26.890592, q1: 26.661210, q2: 27.177651, e: 0.882696\n",
      "I: 22, r: 25.000000, v: 0.026355, q1: 2724.309858, q2: 2725.055244, p: -27.046291\n",
      "v: 26.491542, q1: 25.903174, q2: 26.471828, e: 0.869650\n",
      "I: 23, r: 25.974026, v: 0.026787, q1: 2616.422078, q2: 2617.072328, p: -26.677998\n",
      "v: 26.569201, q1: 26.119015, q2: 26.582392, e: 0.871688\n",
      "I: 24, r: 25.316456, v: 0.020417, q1: 2576.709890, q2: 2577.165441, p: -26.354909\n",
      "v: 25.797701, q1: 25.130617, q2: 25.657373, e: 0.870233\n",
      "I: 25, r: 26.315789, v: 0.021030, q1: 2482.017230, q2: 2482.558936, p: -26.076290\n",
      "v: 25.944107, q1: 25.727895, q2: 26.095404, e: 0.879846\n",
      "I: 26, r: 25.974026, v: 0.020854, q1: 2490.848537, q2: 2491.351064, p: -25.775265\n",
      "v: 25.200502, q1: 25.036693, q2: 25.349379, e: 0.864200\n",
      "I: 27, r: 28.985507, v: 0.018134, q1: 2371.612188, q2: 2372.107176, p: -25.552477\n",
      "v: 25.666194, q1: 25.449528, q2: 25.886266, e: 0.875863\n",
      "I: 28, r: 27.027027, v: 0.018934, q1: 2318.668171, q2: 2319.261020, p: -25.363808\n",
      "v: 24.847642, q1: 24.542009, q2: 24.969469, e: 0.886034\n",
      "I: 29, r: 27.027027, v: 0.028181, q1: 2319.366551, q2: 2320.063391, p: -25.178641\n",
      "v: 25.373978, q1: 24.747369, q2: 25.037879, e: 0.877719\n",
      "I: 30, r: 25.641026, v: 0.020096, q1: 2332.905567, q2: 2333.385266, p: -24.954939\n",
      "v: 24.566373, q1: 24.576402, q2: 24.959680, e: 0.868406\n",
      "I: 31, r: 25.316456, v: 0.021311, q1: 2242.606188, q2: 2243.156478, p: -24.799758\n",
      "v: 24.445254, q1: 24.250300, q2: 24.586515, e: 0.871813\n",
      "I: 32, r: 26.315789, v: 0.023492, q1: 2188.646029, q2: 2189.047852, p: -24.667194\n",
      "v: 24.440936, q1: 24.376181, q2: 24.760746, e: 0.858756\n",
      "I: 33, r: 25.641026, v: 0.023713, q1: 2198.658562, q2: 2199.116541, p: -24.533604\n",
      "v: 24.752038, q1: 24.107498, q2: 24.281251, e: 0.872609\n",
      "I: 34, r: 24.096386, v: 0.026991, q1: 2177.124181, q2: 2177.694468, p: -24.408576\n"
     ]
    }
   ],
   "source": [
    "train!(env, memory, optim, policy, critics, value, value_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test(env, policy)\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for _ in 1:5000\n",
    "        mu, sigma = forward_policy(policy, state)\n",
    "        state, reward, death, _ = env.step(mu)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "        if death\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    total_reward\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(env, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@BSON.save \"models.bson\" Dict(\n",
    "    :value => value,\n",
    "    :value_target => value_target,\n",
    "    :critics => critics,\n",
    "    :policy => policy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.0593887521755381 -0.09837611675025143 … 0.011800205368475647 -0.19544651568152593; -0.7859106243879304 -1.4163999185471952 … 0.02471146324088716 -0.8055429254736141; -0.018139650155244755 0.1221037720307057 … -0.03026086376533901 0.029718731219978547; 0.4925133549958805 1.0070555566413861 … 0.012829738314209124 0.20123916341663578], [8.416118698940394 5.8421170146112615 … 4.956979129541407 8.013060997234167], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [-0.08039334550728447 -0.13071587094306414 … 0.00946937420570149 -0.21736059562231513; -1.0502296665873185 -1.6169877096406347 … -0.1165415581387079 -1.0957039970394593; -0.004989776396819065 0.1445471776105656 … -0.028073328105376015 0.036913824237685744; 0.6574936879212845 1.122170278992995 … 0.10937678299814976 0.35975465088535985], Bool[0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 10000\n",
    "s,a,r,sa,d = batch(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
