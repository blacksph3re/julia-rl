{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Zygote\n",
    "using PyCall\n",
    "using DataStructures\n",
    "using StatsBase\n",
    "using Printf\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <TimeLimit<CartPoleEnv<CartPole-v1>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym = pyimport(\"gym\")\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATE_SPACE = length(env.observation_space.low)\n",
    "ACTION_SPACE = 2\n",
    "ACTIONS = collect(0:(ACTION_SPACE-1))\n",
    "EPSILON_START = 0.50\n",
    "EPSILON_END = 0.05\n",
    "EPOCHS = 50\n",
    "STEPS_PER_EPOCH = 2000\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "DISCOUNT = 0.99\n",
    "L2_REG = 1e-3\n",
    "TARGET_UPDATE = 5e-4\n",
    "LEARNING_RATE = 1e-3\n",
    "LAYER1_SIZE = 20\n",
    "LAYER2_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0-element CircularBuffer{Tuple{Array{Float32,1},Int64,Float32,Array{Float32,1},Bool}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = CircularBuffer{Tuple{Array{Float32,1}, Int64, Float32, Array{Float32,1}, Bool}}(BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "act (generic function with 1 method)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(\n",
    "    Dense(STATE_SPACE + ACTION_SPACE, LAYER1_SIZE, tanh),\n",
    "    LayerNorm(LAYER1_SIZE),\n",
    "    Dense(LAYER1_SIZE, LAYER2_SIZE, tanh),\n",
    "    LayerNorm(LAYER2_SIZE),\n",
    "    Dense(LAYER2_SIZE, 1),\n",
    ")\n",
    "\n",
    "target_model = deepcopy(model)\n",
    "\n",
    "function forward(model, state, action::Int)\n",
    "    model(vcat(state, Flux.onehot(action, ACTIONS)))\n",
    "end\n",
    "\n",
    "function forward(model, state, action::AbstractArray)\n",
    "    model(vcat(state, Flux.onehotbatch(action, ACTIONS)))\n",
    "end\n",
    "\n",
    "function q_values(model, state)\n",
    "    tmp = map((a) -> forward(model, state, ndims(state) == 1 ? a : fill(a, size(state)[2])), ACTIONS)\n",
    "    reduce(vcat, tmp)\n",
    "end\n",
    "\n",
    "function valueest(model, state::AbstractArray)\n",
    "    return maximum(q_values(model, state), dims=1)\n",
    "end\n",
    "\n",
    "# Act after an epsilon-greedy strategy\n",
    "function act(model, state, epsilon)\n",
    "    if epsilon != 0 && rand() <= epsilon\n",
    "        return rand(ACTIONS)\n",
    "    end\n",
    "    tmp = ACTIONS[map((idx) -> idx[1], argmax(q_values(model, state), dims=1))]\n",
    "    ndims(tmp) == 1 ? tmp[1] : tmp\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.001, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim = ADAM(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_descend! (generic function with 1 method)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function grad_descend!(model, parameters, target_model, memory)\n",
    "    outer_loss = 0\n",
    "    # Sample BATCH_SIZE elements from the replay buffer\n",
    "    batch = memory[1:BATCH_SIZE]\n",
    "    sample!(memory, batch, replace=false)\n",
    "    # Destructure the batch\n",
    "    state, action, reward, next_state, death = [getindex.(batch, i) for i in 1:5]\n",
    "    state = hcat(state...)\n",
    "    next_state = hcat(next_state...)\n",
    "\n",
    "    # Perform one step of grad descend\n",
    "    Q_next = .!death .* valueest(target_model, next_state)\n",
    "    target = reward .+ DISCOUNT .* Q_next\n",
    "    \n",
    "    grads = gradient(parameters) do\n",
    "        Q_pred = forward(model, state, action)\n",
    "        loss = Flux.huber_loss(Q_pred, target) + L2_REG * sum(norm, parameters)\n",
    "        outer_loss += loss\n",
    "        return loss\n",
    "    end\n",
    "\n",
    "    Flux.update!(optim, parameters, grads)\n",
    "        \n",
    "    for (p, p_target) in zip(parameters, Flux.params(target_model))\n",
    "       p_target .= (1-TARGET_UPDATE) .* p_target .+ TARGET_UPDATE .* p\n",
    "    end\n",
    "    \n",
    "    return outer_loss, mean(target)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_epoch! (generic function with 1 method)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_epoch!(memory, model, target_model, env, epoch)\n",
    "    state = Float32.(env.reset())\n",
    "    total_reward = 0\n",
    "    total_deaths = 0\n",
    "    epsilon = EPSILON_START * (1 - epoch/EPOCHS) + EPSILON_END * (epoch/EPOCHS)\n",
    "    \n",
    "    parameters = Flux.params(model)\n",
    "    total_loss = 0.0\n",
    "    total_q = 0.0\n",
    "    total_q_target = 0.0\n",
    "    \n",
    "   \n",
    "    for step in 1:STEPS_PER_EPOCH\n",
    "        action = act(model, state, epsilon) # Act\n",
    "        next_state, reward, death, _ = env.step(action) # Advance the env\n",
    "        \n",
    "        # Convert to Float32\n",
    "        next_state = Float32.(next_state)\n",
    "        reward = Float32(reward)\n",
    "        \n",
    "        push!(memory, (state, action, reward, next_state, death)) # Save that memory\n",
    "        total_reward += reward\n",
    "        if death\n",
    "            state = env.reset()\n",
    "            total_deaths += 1\n",
    "        else\n",
    "            state = next_state\n",
    "        end\n",
    "        \n",
    "        if length(memory) > BATCH_SIZE\n",
    "            loss, q_target = grad_descend!(model, parameters, target_model, memory)\n",
    "            total_loss += loss\n",
    "            total_q_target += q_target\n",
    "        end\n",
    "    end\n",
    "    return total_reward / (total_deaths + 1), total_loss / STEPS_PER_EPOCH, total_q_target / STEPS_PER_EPOCH\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.202699, Reward 14.705882, target 3.846654\n",
      "Epoch 2, Loss 0.267166, Reward 17.391304, target 5.117557\n",
      "Epoch 3, Loss 0.311139, Reward 22.222221, target 6.203709\n",
      "Epoch 4, Loss 0.325515, Reward 45.454544, target 7.202230\n",
      "Epoch 5, Loss 0.324456, Reward 51.282051, target 8.181096\n",
      "Epoch 6, Loss 0.308244, Reward 38.461540, target 9.139338\n",
      "Epoch 7, Loss 0.281239, Reward 35.714287, target 10.102285\n",
      "Epoch 8, Loss 0.274610, Reward 28.571428, target 11.043230\n",
      "Epoch 9, Loss 0.293690, Reward 24.691359, target 11.950754\n",
      "Epoch 10, Loss 0.379699, Reward 22.471910, target 12.730969\n",
      "Epoch 11, Loss 0.473040, Reward 19.047619, target 13.480741\n",
      "Epoch 12, Loss 0.569868, Reward 18.348623, target 14.211634\n",
      "Epoch 13, Loss 0.675977, Reward 18.867924, target 14.922304\n",
      "Epoch 14, Loss 0.761607, Reward 19.230770, target 15.639124\n",
      "Epoch 15, Loss 0.810820, Reward 21.739130, target 16.377488\n",
      "Epoch 16, Loss 0.804513, Reward 27.397261, target 17.150451\n",
      "Epoch 17, Loss 0.848867, Reward 14.184397, target 17.878196\n",
      "Epoch 18, Loss 0.992568, Reward 12.269938, target 18.487229\n",
      "Epoch 19, Loss 1.123251, Reward 13.071896, target 19.095616\n",
      "Epoch 20, Loss 1.247326, Reward 14.492754, target 19.701011\n",
      "Epoch 21, Loss 1.392795, Reward 15.384615, target 20.271830\n",
      "Epoch 22, Loss 1.532260, Reward 13.245033, target 20.836127\n",
      "Epoch 23, Loss 1.568009, Reward 12.422360, target 21.498422\n",
      "Epoch 24, Loss 1.617240, Reward 12.987013, target 22.141888\n",
      "Epoch 25, Loss 1.656391, Reward 12.987013, target 22.790052\n",
      "Epoch 26, Loss 1.786336, Reward 14.388489, target 23.336175\n",
      "Epoch 27, Loss 1.825595, Reward 13.157895, target 23.966907\n",
      "Epoch 28, Loss 1.882173, Reward 13.071896, target 24.572247\n",
      "Epoch 29, Loss 1.906886, Reward 13.888889, target 25.202180\n",
      "Epoch 30, Loss 1.907427, Reward 14.705882, target 25.851996\n",
      "Epoch 31, Loss 1.975415, Reward 12.500000, target 26.430211\n",
      "Epoch 32, Loss 2.068460, Reward 12.345679, target 26.976720\n",
      "Epoch 33, Loss 2.145431, Reward 12.345679, target 27.530902\n",
      "Epoch 34, Loss 2.212583, Reward 12.422360, target 28.086156\n",
      "Epoch 35, Loss 2.334543, Reward 12.422360, target 28.575737\n",
      "Epoch 36, Loss 2.437240, Reward 13.245033, target 29.076817\n",
      "Epoch 37, Loss 2.435722, Reward 13.071896, target 29.681175\n",
      "Epoch 38, Loss 2.480045, Reward 11.627907, target 30.235051\n",
      "Epoch 39, Loss 2.507051, Reward 12.578616, target 30.797809\n",
      "Epoch 40, Loss 2.546765, Reward 11.834319, target 31.342545\n",
      "Epoch 41, Loss 2.702555, Reward 11.363636, target 31.761570\n",
      "Epoch 42, Loss 2.800922, Reward 11.976048, target 32.232916\n",
      "Epoch 43, Loss 2.835408, Reward 11.363636, target 32.763564\n",
      "Epoch 44, Loss 2.996740, Reward 10.989011, target 33.156562\n",
      "Epoch 45, Loss 3.101118, Reward 11.764706, target 33.597809\n",
      "Epoch 46, Loss 3.067055, Reward 11.834319, target 34.177333\n",
      "Epoch 47, Loss 3.114208, Reward 12.121212, target 34.668559\n",
      "Epoch 48, Loss 3.224480, Reward 10.204082, target 35.087040\n",
      "Epoch 49, Loss 3.249328, Reward 11.111111, target 35.587361\n",
      "Epoch 50, Loss 3.291962, Reward 12.578616, target 36.068634\n"
     ]
    }
   ],
   "source": [
    "for e in 1:EPOCHS\n",
    "    loss, reward, target = train_epoch!(memory, model, target_model, env, e)\n",
    "    println(@sprintf(\"Epoch %d, Loss %f, Reward %f, target %f\", e, reward, loss, target))\n",
    "    if reward > 200\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test (generic function with 1 method)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test(model, env)\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for i in 1:STEPS_PER_EPOCH\n",
    "        action = act(model, state, 0)\n",
    "        state, reward, death, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        env.render()\n",
    "        death && return total_reward, i\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129.0, 129)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(target_model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float32,1}:\n",
       " 10.167985\n",
       " 10.290317"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "s = 0\n",
    "r = 0\n",
    "d = 0\n",
    "for _ in 1:5\n",
    "    s,r,d = env.step(1)\n",
    "end\n",
    "valueest(model, s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
